{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Anish Work:\n",
    "- Was able to understand how lxml works\n",
    "- Changed the website to scrape the aggregate reviews website\n",
    "- fixed xpath for reviews for aggregate review page\n",
    "- added and implemented review count into json\n",
    "Need to still do:\n",
    "- A for loop that will iterate through the pages in the aggregate reviews website\n",
    "\n",
    "   (https://www.amazon.com/product-reviews/B01DFKC2SO/ref=cm_cr_getr_d_paging_btm_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1)\n",
    "   \n",
    "   change the \"...pageNumber=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from json import dump,loads\n",
    "from requests import get\n",
    "import json\n",
    "from re import sub\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and processing page http://www.amazon.com/dp/B01DFKC2SO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and processing page http://www.amazon.com/dp/B0792KTHKJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and processing page http://www.amazon.com/dp/B06XCM9LJ4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "def ParseReviews(asin):\n",
    "    # This script has only been tested with Amazon.com\n",
    "    \n",
    "    amazon_url  = ('https://www.amazon.com/product-reviews/{0}/ref=cm_cr_getr_d_paging_btm_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1'\n",
    "    .format(asin)) #asin is unique amazon id\n",
    "    \n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'}\n",
    "    for i in range(5):\n",
    "        response = get(amazon_url, headers = headers, verify=False, timeout=30)\n",
    "        if response.status_code == 404:\n",
    "            return {\"url\": amazon_url, \"error\": \"page not found\"}\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "        \n",
    "        # Removing the null bytes from the response.\n",
    "        cleaned_response = response.text.replace('\\x00', '')\n",
    "        \n",
    "        parser = html.fromstring(cleaned_response)\n",
    "        XPATH_AGGREGATE = '//span[@id=\"acrCustomerReviewText\"]'\n",
    "        XPATH_REVIEW_SECTION_1 = '//div[contains(@id,\"reviews-summary\")]'\n",
    "        XPATH_REVIEW_SECTION_2 = '//div[@data-hook=\"review\"]'\n",
    "        XPATH_AGGREGATE_RATING = '//table[@id=\"histogramTable\"]//tr'\n",
    "        XPATH_PRODUCT_NAME = '//h1//span[@id=\"productTitle\"]//text()'\n",
    "        XPATH_PRODUCT_PRICE = '//span[@id=\"priceblock_ourprice\"]/text()'\n",
    "\n",
    "        raw_product_price = parser.xpath(XPATH_PRODUCT_PRICE)\n",
    "        raw_product_name = parser.xpath(XPATH_PRODUCT_NAME)\n",
    "        total_ratings  = parser.xpath(XPATH_AGGREGATE_RATING)\n",
    "        reviews = parser.xpath(XPATH_REVIEW_SECTION_1)\n",
    "\n",
    "        product_price = ''.join(raw_product_price).replace(',', '')\n",
    "        product_name = ''.join(raw_product_name).strip()\n",
    "\n",
    "        if not reviews:\n",
    "            reviews = parser.xpath(XPATH_REVIEW_SECTION_2)\n",
    "        ratings_dict = {}\n",
    "        reviews_list = []\n",
    "\n",
    "        # Grabing the rating  section in product page\n",
    "        for ratings in total_ratings:\n",
    "            extracted_rating = ratings.xpath('./td//a//text()')\n",
    "            if extracted_rating:\n",
    "                rating_key = extracted_rating[0] \n",
    "                raw_raing_value = extracted_rating[1]\n",
    "                rating_value = raw_raing_value\n",
    "                if rating_key:\n",
    "                    ratings_dict.update({rating_key: rating_value})\n",
    "        \n",
    "        # Parsing individual reviews\n",
    "        for review in reviews:\n",
    "            XPATH_RATING  = './/i[@data-hook=\"review-star-rating\"]//text()'\n",
    "            XPATH_REVIEW_HEADER = './/a[@data-hook=\"review-title\"]//text()'\n",
    "            XPATH_REVIEW_POSTED_DATE = './/span[@data-hook=\"review-date\"]//text()'\n",
    "            XPATH_REVIEW_TEXT_1 = './/div[@class=\"a-row a-spacing-small review-data\"]//text()'\n",
    "            XPATH_REVIEW_TEXT_2 = './/div//span[@data-action=\"columnbalancing-showfullreview\"]/@data-columnbalancing-showfullreview'\n",
    "            XPATH_REVIEW_COMMENTS = './/span[@class=\"a-size-base\"]//text()'\n",
    "            XPATH_AUTHOR = './/span[contains(@class,\"profile-name\")]//text()'\n",
    "            XPATH_REVIEW_TEXT_3 = './/div[contains(@id,\"dpReviews\")]/div/text()'\n",
    "            XPATH_HELPFUL_VOTE_DIV = './/div[@class=\"a-row a-spacing-small\"]'\n",
    "            TEST_XPATH = './/div[@class=\"a-size-base a-color-tertiary cr-vote-text\"]//text()'\n",
    "            \n",
    "            raw_review_author = review.xpath(XPATH_AUTHOR)\n",
    "            raw_review_rating = review.xpath(XPATH_RATING)\n",
    "            raw_review_header = review.xpath(XPATH_REVIEW_HEADER)\n",
    "            raw_review_posted_date = review.xpath(XPATH_REVIEW_POSTED_DATE)\n",
    "            raw_review_text1 = review.xpath(XPATH_REVIEW_TEXT_1)\n",
    "            raw_review_text2 = review.xpath(XPATH_REVIEW_TEXT_2)\n",
    "            raw_review_text3 = review.xpath(XPATH_REVIEW_TEXT_3)\n",
    "            raw_helpful = review.xpath(XPATH_HELPFUL_VOTE_DIV)[0].text_content().split(' ')[0]\n",
    "            raw_review_count = review.xpath(XPATH_REVIEW_COMMENTS)[0][0]\n",
    "\n",
    "            # Cleaning data\n",
    "            author = ' '.join(' '.join(raw_review_author).split())\n",
    "            review_rating = ''.join(raw_review_rating).replace('out of 5 stars', '')\n",
    "            review_header = ' '.join(' '.join(raw_review_header).split())\n",
    "\n",
    "            try:\n",
    "                review_posted_date = dateparser.parse(''.join(raw_review_posted_date)).strftime('%d %b %Y')\n",
    "            except:\n",
    "                review_posted_date = None\n",
    "            review_text = ' '.join(' '.join(raw_review_text1).split())\n",
    "\n",
    "            # Grabbing hidden comments if present\n",
    "            if raw_review_text2:\n",
    "                json_loaded_review_data = loads(raw_review_text2[0])\n",
    "                json_loaded_review_data_text = json_loaded_review_data['rest']\n",
    "                cleaned_json_loaded_review_data_text = re.sub('<.*?>', '', json_loaded_review_data_text)\n",
    "                full_review_text = review_text+cleaned_json_loaded_review_data_text\n",
    "            else:\n",
    "                full_review_text = review_text\n",
    "            if not raw_review_text1:\n",
    "                full_review_text = ' '.join(' '.join(raw_review_text3).split())\n",
    "\n",
    "            raw_review_comments = review.xpath(XPATH_REVIEW_COMMENTS)\n",
    "            review_comments = ''.join(raw_review_comments)\n",
    "            review_comments = sub('[A-Za-z]', '', review_comments).strip()\n",
    "            review_dict = {\n",
    "                                'review_comment_count': raw_review_count,\n",
    "                                'review_helpful' : raw_helpful,\n",
    "                                'review_text': full_review_text,\n",
    "                                'review_posted_date': review_posted_date,\n",
    "                                'review_header': review_header,\n",
    "                                'review_rating': review_rating,\n",
    "                                'review_author': author\n",
    "\n",
    "                            }\n",
    "            reviews_list.append(review_dict)\n",
    "\n",
    "        data = {\n",
    "                    'ratings': ratings_dict,\n",
    "                    'reviews': reviews_list,\n",
    "                    'url': amazon_url,\n",
    "                    'name': product_name,\n",
    "                    'price': product_price\n",
    "                \n",
    "                }\n",
    "        return data\n",
    "\n",
    "    return {\"error\": \"failed to process the page\", \"url\": amazon_url}\n",
    "            \n",
    "\n",
    "def ReadAsin():\n",
    "    # Add your own ASINs here\n",
    "    AsinList = ['B01DFKC2SO', 'B0792KTHKJ', 'B06XCM9LJ4']\n",
    "    extracted_data = []\n",
    "    \n",
    "    for asin in AsinList:\n",
    "        print(\"Downloading and processing page http://www.amazon.com/dp/\" + asin)\n",
    "        extracted_data.append(ParseReviews(asin))\n",
    "        sleep(5)\n",
    "    f = open('data.json', 'w')\n",
    "    dump(extracted_data, f, indent=4)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ReadAsin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
