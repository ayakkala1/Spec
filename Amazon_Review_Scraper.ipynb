{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Scraper    \n",
    "    \n",
    "    Anish Work:\n",
    "- Was able to understand how lxml works\n",
    "- Changed the website to scrape the aggregate reviews website\n",
    "- fixed xpath for reviews for aggregate review page\n",
    "- added and implemented review count into json\n",
    "- A for loop that will iterate through the pages in the aggregate reviews website\n",
    "- Added try catch for rating, in edge case that nobody found the review helpful\n",
    "\n",
    "   (https://www.amazon.com/product-reviews/B01DFKC2SO/ref=cm_cr_getr_d_paging_btm_1?ie=UTF8&reviewerType=all_reviews&pageNumber=1)\n",
    "   \n",
    "   change the \"...pageNumber=#\n",
    "   \n",
    "- Fixed: Was able to scrape the name from the review page instead of the main product name, reducing run time\n",
    "   \n",
    "## Need to do:\n",
    "\n",
    "- Fix Comments Scrape\n",
    "\n",
    "- Figure out certainly if Amazon url reviews in the hundreds change.\n",
    "\n",
    "   \n",
    "## How to use:\n",
    "\n",
    "- Go to the function ReadAsin and add your asin code to the list.\n",
    "\n",
    "    (In the url example above it is the ...B01DFKC2SO...)\n",
    "\n",
    "- Change pg_amounts to how many pages you want to iterate over\n",
    "\n",
    "- If things stop working, it's probably because Amazon caught on.\n",
    "    \n",
    "    Go to...\n",
    "\n",
    "    https://developers.whatismybrowser.com/useragents/explore/software_name/chrome/\n",
    "    \n",
    "    Pick a user string then go to the headers variable in ParseReviews and change the string after User Agent:\n",
    "\n",
    "- After running the kernel the scraped data will be sent to the file name you chose + .json\n",
    "\n",
    "- If you appended the data to a dataset please make sure  to....\n",
    "\n",
    "    AFTER RUNNING GO TO THE JSON FILE AND MANUALLY FIND WHERE YOU APPENDED IT AND MAKE IT SO IT BECOMES ONE BIG LIST.\n",
    "    WILL HAVE TO DO UNTIL I FIGURE OUT HOW TO IMPORT THE MULTIPLE JSON LISTS AS ONE INTO PANDAS. \n",
    "    \n",
    "    This can be a bit confusing so ask me for help if needed.\n",
    "    \n",
    "  \n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from json import dump,loads\n",
    "from requests import get\n",
    "import json\n",
    "from re import sub\n",
    "from dateutil import parser as dateparser\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own ASINs here\n",
    "\n",
    "AsinList = ['B01DFKC2SO', 'B0792KTHKJ', 'B06XCM9LJ4']\n",
    "\n",
    "# Add your pg amounts here\n",
    "\n",
    "pg_amounts = 1000 \n",
    "\n",
    "# Add the file name here\n",
    "\n",
    "file = 'test.json'\n",
    "\n",
    "# Add file read mode here (\"w\" - > write mode, \"a+\" - > append mode)\n",
    "\n",
    "mode = \"w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseReviews(asin,pgnumber):\n",
    "    # This script has only been tested with Amazon.com\n",
    "    \n",
    "    amazon_url  = ('https://www.amazon.com/All-New-Amazon-Echo-Dot-Add-Alexa-To-Any-Room/product-reviews/{0}/ref=cm_cr_arp_d_paging_btm_2?ie=UTF8&reviewerType=all_reviews&pageNumber={1}'\n",
    "    .format(asin,pgnumber)) #asin is unique amazon id\n",
    "    \n",
    "    # Add some recent user agent to prevent amazon from blocking the request \n",
    "    # Find some chrome user agent strings  here https://udger.com/resources/ua-list/browser-detail?browser=Chrome\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.13 (KHTML, like Gecko) Chrome/9.0.597.0 Safari/534.13'}\n",
    "    for i in range(1):\n",
    "        response = get(amazon_url, headers = headers, verify=False, timeout=30)\n",
    "        if response.status_code == 404:\n",
    "            return {\"url\": amazon_url, \"error\": \"page not found\"}\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "        \n",
    "        # Removing the null bytes from the response.\n",
    "        cleaned_response = response.text.replace('\\x00', '')\n",
    "\n",
    "        parser = html.fromstring(cleaned_response)\n",
    "        XPATH_AGGREGATE = '//span[@id=\"acrCustomerReviewText\"]'\n",
    "        XPATH_REVIEW_SECTION_1 = '//div[contains(@id,\"reviews-summary\")]'\n",
    "        XPATH_REVIEW_SECTION_2 = '//div[@data-hook=\"review\"]'\n",
    "        XPATH_AGGREGATE_RATING = '//table[@id=\"histogramTable\"]//tr'\n",
    "        XPATH_PRODUCT_NAME = '//h1[@class=\"a-size-large a-text-ellipsis\"]//text()'\n",
    "        XPATH_PRICE = './/span[@class=\"a-color-price arp-price\"]'\n",
    "        try:\n",
    "            raw_product_price = parser.xpath(XPATH_PRICE)[0].text_content().split(' ')[-1]\n",
    "        except:\n",
    "            raw_product_price = \"0\"\n",
    "        raw_product_name = parser.xpath(XPATH_PRODUCT_NAME)[0]\n",
    "        total_ratings  = parser.xpath(XPATH_AGGREGATE_RATING)\n",
    "        reviews = parser.xpath(XPATH_REVIEW_SECTION_1)\n",
    "\n",
    "        product_price = ''.join(raw_product_price).replace(',', '')\n",
    "        product_name = ''.join(raw_product_name).strip()\n",
    "\n",
    "        if not reviews:\n",
    "            reviews = parser.xpath(XPATH_REVIEW_SECTION_2)\n",
    "        ratings_dict = {}\n",
    "        reviews_list = []\n",
    "\n",
    "        # Grabing the rating  section in product page\n",
    "        for ratings in total_ratings:\n",
    "            extracted_rating = ratings.xpath('./td//a//text()')\n",
    "            if extracted_rating:\n",
    "                rating_key = extracted_rating[0] \n",
    "                raw_raing_value = extracted_rating[1]\n",
    "                rating_value = raw_raing_value\n",
    "                if rating_key:\n",
    "                    ratings_dict.update({rating_key: rating_value})\n",
    "        \n",
    "        # Parsing individual reviews\n",
    "        for review in reviews:\n",
    "            XPATH_RATING  = './/i[@data-hook=\"review-star-rating\"]//text()'\n",
    "            XPATH_REVIEW_HEADER = './/a[@data-hook=\"review-title\"]//text()'\n",
    "            XPATH_REVIEW_POSTED_DATE = './/span[@data-hook=\"review-date\"]//text()'\n",
    "            XPATH_REVIEW_TEXT_1 = './/div[@class=\"a-row a-spacing-small review-data\"]//text()'\n",
    "            XPATH_REVIEW_TEXT_2 = './/div//span[@data-action=\"columnbalancing-showfullreview\"]/@data-columnbalancing-showfullreview'\n",
    "            XPATH_REVIEW_COMMENTS = './/span[@class=\"a-size-base\"]//text()'\n",
    "            XPATH_AUTHOR = './/span[contains(@class,\"profile-name\")]//text()'\n",
    "            XPATH_REVIEW_TEXT_3 = './/div[contains(@id,\"dpReviews\")]/div/text()'\n",
    "            XPATH_HELPFUL_VOTE_DIV = './/div[@class=\"a-row a-spacing-small\"]//text()'\n",
    "            TEST_XPATH = './/div[@class=\"a-size-base a-color-tertiary cr-vote-text\"]//text()'\n",
    "            \n",
    "            raw_review_author = review.xpath(XPATH_AUTHOR)\n",
    "            raw_review_rating = review.xpath(XPATH_RATING)\n",
    "            raw_review_header = review.xpath(XPATH_REVIEW_HEADER)\n",
    "            raw_review_posted_date = review.xpath(XPATH_REVIEW_POSTED_DATE)\n",
    "            raw_review_text1 = review.xpath(XPATH_REVIEW_TEXT_1)\n",
    "            raw_review_text2 = review.xpath(XPATH_REVIEW_TEXT_2)\n",
    "            raw_review_text3 = review.xpath(XPATH_REVIEW_TEXT_3)\n",
    "            try: # Case when nobody finds it useful\n",
    "                raw_helpful = review.xpath(XPATH_HELPFUL_VOTE_DIV)[0].split(' ')[0]\n",
    "            except:\n",
    "                raw_helpful = 0\n",
    "            raw_review_count = review.xpath(XPATH_REVIEW_COMMENTS)[0][0]\n",
    "\n",
    "            # Cleaning data\n",
    "            author = ' '.join(' '.join(raw_review_author).split())\n",
    "            review_rating = ''.join(raw_review_rating).replace('out of 5 stars', '')\n",
    "            review_header = ' '.join(' '.join(raw_review_header).split())\n",
    "\n",
    "            try:\n",
    "                review_posted_date = dateparser.parse(''.join(raw_review_posted_date)).strftime('%d %b %Y')\n",
    "            except:\n",
    "                review_posted_date = None\n",
    "            review_text = ' '.join(' '.join(raw_review_text1).split())\n",
    "\n",
    "            # Grabbing hidden comments if present\n",
    "            if raw_review_text2:\n",
    "                json_loaded_review_data = loads(raw_review_text2[0])\n",
    "                json_loaded_review_data_text = json_loaded_review_data['rest']\n",
    "                cleaned_json_loaded_review_data_text = re.sub('<.*?>', '', json_loaded_review_data_text)\n",
    "                full_review_text = review_text+cleaned_json_loaded_review_data_text\n",
    "            else:\n",
    "                full_review_text = review_text\n",
    "            if not raw_review_text1:\n",
    "                full_review_text = ' '.join(' '.join(raw_review_text3).split())\n",
    "\n",
    "            raw_review_comments = review.xpath(XPATH_REVIEW_COMMENTS)\n",
    "            review_comments = ''.join(raw_review_comments)\n",
    "            review_comments = sub('[A-Za-z]', '', review_comments).strip()\n",
    "            review_dict = {\n",
    "                                'review_comment_count': raw_review_count,\n",
    "                                'review_helpful' : raw_helpful,\n",
    "                                'review_text': full_review_text,\n",
    "                                'review_posted_date': review_posted_date,\n",
    "                                'review_header': review_header,\n",
    "                                'review_rating': review_rating,\n",
    "                                'review_author': author\n",
    "\n",
    "                            }\n",
    "            reviews_list.append(review_dict)\n",
    "\n",
    "        data = {\n",
    "                    'ratings': ratings_dict,\n",
    "                    'reviews': reviews_list,\n",
    "                    'url': amazon_url,\n",
    "                    'name': raw_product_name,\n",
    "                    'price': product_price\n",
    "                \n",
    "                }\n",
    "        return data\n",
    "\n",
    "    return {\"error\": \"failed to process the page\", \"url\": amazon_url}\n",
    "\n",
    "\n",
    "def ReadAsin(AsinList, pg_amounts, file, mode):\n",
    "    try:\n",
    "        for asin in AsinList:\n",
    "            print(\"Downloading and processing page http://www.amazon.com/dp/\" + asin)\n",
    "            for i in range(1,pg_amounts+1):\n",
    "                print(\"Iteration \",i)\n",
    "                extracted_data.append(ParseReviews(asin,i))\n",
    "                sleep(5)\n",
    "    except:\n",
    "        extracted_data.append(ParseReviews(asin,i))\n",
    "    f = open(file, mode)\n",
    "    dump(extracted_data, f, indent=4)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and processing page http://www.amazon.com/dp/B01DFKC2SO\n",
      "Iteration  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "extracted_data = []\n",
    "ReadAsin(AsinList,pg_amounts,file, mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
